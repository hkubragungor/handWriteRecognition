{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwhNdLxvZScT",
        "colab_type": "code",
        "outputId": "87d44bf3-98e0-42e2-e2a6-bc287488ef8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import keras\n",
        "import numpy as np\n",
        "from keras.utils import to_categorical\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, Dense,Conv2D,MaxPooling2D\n",
        "from keras.layers import Dropout, Flatten\n",
        "from keras.utils import np_utils\n",
        "from keras import optimizers\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "import random\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcL9fej3dl5o",
        "colab_type": "text"
      },
      "source": [
        " **if you're upload your zipped dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF46D7Un5U2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import zipfile\n",
        "zip_ref = zipfile.ZipFile('drive/My Drive/dataset_filepath.zip', 'r')\n",
        "zip_ref.extractall('drive/My Drive/Colab Notebooks')\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_06IfgLZd_WG",
        "colab_type": "text"
      },
      "source": [
        "***import dataset as numpy array*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bd9LpXZkZnIf",
        "colab_type": "code",
        "outputId": "09907f57-7689-467c-8453-0f4f728b899d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "x_train = np.load(\"drive/My Drive/Colab Notebooks/dataset.npy\")\n",
        "y = np.load(\"drive/My Drive/Colab Notebooks/label.npy\")\n",
        "x_train = x_train.reshape(-1,32, 32, 1)\n",
        "classes = np.unique(y)\n",
        "nClasses =len(classes)\n",
        "y_train = keras.utils.to_categorical(y,num_classes=nClasses)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)\n",
        "print(y[0])\n",
        "print(len(x_train))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of outputs :  42\n",
            "Output classes :  [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            " 18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            " 36. 37. 38. 39. 40. 41.]\n",
            "37.0\n",
            "752215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0noo2CNLevF8",
        "colab_type": "text"
      },
      "source": [
        "**Model Creating**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVcbtkK_ZuyU",
        "colab_type": "code",
        "outputId": "167fbe73-f64b-4c5a-c4e7-8824bebc2608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=[32,32,1]))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3),activation='relu',padding='same'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(288, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(144, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(72, activation='relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(nClasses, activation='softmax'))\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 30, 30, 32)        320       \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 30, 30, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 15, 15, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 15, 15, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 7, 7, 64)          18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 7, 7, 64)          256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_12 (MaxPooling (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 288)               166176    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 288)               1152      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 288)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 144)               41616     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 144)               576       \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 144)               0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 72)                10440     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 72)                288       \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 72)                0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 42)                3066      \n",
            "=================================================================\n",
            "Total params: 251,890\n",
            "Trainable params: 250,626\n",
            "Non-trainable params: 1,264\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHzzNbdOe4RP",
        "colab_type": "text"
      },
      "source": [
        "**This project we using multi-label classification and we use k-fold cross-validation for getting good accuracy "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7h28Lq0bDkQ",
        "colab_type": "code",
        "outputId": "7a06492a-f5a5-481f-ac9b-723e13bf9693",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4389
        }
      },
      "source": [
        "c = list(zip(x_train,y_train))\n",
        "random.shuffle(c)\n",
        "x_train,y_train = zip(*c)\n",
        "del c\n",
        "k=10\n",
        "length = len(x_train)\n",
        "kfoldLen=int(length/k)\n",
        "score = []\n",
        "\n",
        "for x in range(k):\n",
        "  testX=[]\n",
        "  testY=[]\n",
        "  trainX=[]\n",
        "  trainY=[]\n",
        "\n",
        "  split_init = kfoldLen*x\n",
        "  split_end = split_init+kfoldLen\n",
        "  testX = x_train[split_init:split_end]\n",
        "  testY = y_train[split_init:split_end]\n",
        "  if x!=0:\n",
        "    trainX = x_train[0:split_init]\n",
        "    trainY = y_train[0:split_init]\n",
        "    if x!=k-1:\n",
        "      trainX+=x_train[split_end:length-1]\n",
        "      trainY+=y_train[split_end:length-1]\n",
        "  else:\n",
        "    trainX=x_train[split_end:length-1]\n",
        "    trainY=y_train[split_end:length-1]\n",
        "   \n",
        "  trainX = np.asarray(trainX)\n",
        "  trainY = np.asarray(trainY)\n",
        "  testX = np.asarray(testX)\n",
        "  testY = np.asarray(testY)\n",
        "  \n",
        "  opt = keras.optimizers.Adam(lr=0.001)\n",
        "  print(\"------Fold: \"+str(x+1)+\"------\")\n",
        "  model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])\n",
        "  history = model.fit(trainX,trainY,epochs=10,shuffle=False,verbose=1,batch_size=256)\n",
        "  foldScore = model.evaluate(testX,testY)\n",
        "  score.append(foldScore[1]*100)\n",
        "  print(\"-------------Test Score ----------\")\n",
        "  print(\"Accuracy: \"+str(foldScore[1]*100))  \n",
        "  print(\"Loss: \"+str(foldScore[0]*100)) \n",
        "  \n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(score), np.std(score)))\n",
        "\n",
        "model.save(\"drive/My Drive/Colab Notebooks/hndwrte_recog_model.h5\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0618 12:13:23.867762 139758212724608 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0618 12:13:24.008913 139758212724608 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "------Fold: 1------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 91s 134us/step - loss: 0.4351 - acc: 0.8678\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.2690 - acc: 0.9111\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 85s 125us/step - loss: 0.2437 - acc: 0.9185\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.2296 - acc: 0.9223\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.2181 - acc: 0.9257\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.2113 - acc: 0.9275\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.2042 - acc: 0.9297\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1989 - acc: 0.9310\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1946 - acc: 0.9322\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1904 - acc: 0.9336\n",
            "75221/75221 [==============================] - 11s 145us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 93.07640153680488\n",
            "Loss: 20.35077596470937\n",
            "------Fold: 2------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 87s 129us/step - loss: 0.1908 - acc: 0.9334\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1860 - acc: 0.9348\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.1821 - acc: 0.9359\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1793 - acc: 0.9367\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1769 - acc: 0.9375\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.1741 - acc: 0.9381\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1712 - acc: 0.9390\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1691 - acc: 0.9396\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1670 - acc: 0.9405\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1653 - acc: 0.9408\n",
            "75221/75221 [==============================] - 11s 150us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 93.42603794177\n",
            "Loss: 19.077866103776127\n",
            "------Fold: 3------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 88s 130us/step - loss: 0.1680 - acc: 0.9403\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1659 - acc: 0.9408\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1636 - acc: 0.9413\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1615 - acc: 0.9419\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.1592 - acc: 0.9426\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1580 - acc: 0.9428\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1565 - acc: 0.9435\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1547 - acc: 0.9441\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1536 - acc: 0.9443\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1521 - acc: 0.9446\n",
            "75221/75221 [==============================] - 12s 155us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 93.91659244094069\n",
            "Loss: 17.790615834429268\n",
            "------Fold: 4------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 89s 131us/step - loss: 0.1553 - acc: 0.9440\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1535 - acc: 0.9444\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1516 - acc: 0.9450\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1508 - acc: 0.9451\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1491 - acc: 0.9459\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1479 - acc: 0.9458\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1464 - acc: 0.9467\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1453 - acc: 0.9465\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1448 - acc: 0.9471\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1434 - acc: 0.9475\n",
            "75221/75221 [==============================] - 12s 157us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 94.18779330263997\n",
            "Loss: 16.387080752881154\n",
            "------Fold: 5------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 89s 132us/step - loss: 0.1471 - acc: 0.9461\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1446 - acc: 0.9470\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1432 - acc: 0.9474\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1424 - acc: 0.9475\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1417 - acc: 0.9478\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1400 - acc: 0.9484\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1394 - acc: 0.9485\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1390 - acc: 0.9485\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1374 - acc: 0.9492\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1368 - acc: 0.9494\n",
            "75221/75221 [==============================] - 12s 162us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 94.63181824232542\n",
            "Loss: 15.076685046329649\n",
            "------Fold: 6------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 89s 131us/step - loss: 0.1397 - acc: 0.9488\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1383 - acc: 0.9490\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1368 - acc: 0.9494\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1361 - acc: 0.9497\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.1346 - acc: 0.9502\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1345 - acc: 0.9503\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1332 - acc: 0.9505\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.1330 - acc: 0.9504\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 86s 126us/step - loss: 0.1317 - acc: 0.9511\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 85s 126us/step - loss: 0.1309 - acc: 0.9512\n",
            "75221/75221 [==============================] - 12s 163us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 94.74747743332209\n",
            "Loss: 14.499504156070955\n",
            "------Fold: 7------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 89s 132us/step - loss: 0.1345 - acc: 0.9503\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1326 - acc: 0.9507\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1315 - acc: 0.9511\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1310 - acc: 0.9515\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1302 - acc: 0.9517\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1292 - acc: 0.9519\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1285 - acc: 0.9521\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1279 - acc: 0.9525\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1277 - acc: 0.9523\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1270 - acc: 0.9527\n",
            "75221/75221 [==============================] - 13s 169us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 94.91232501562064\n",
            "Loss: 13.736432066206305\n",
            "------Fold: 8------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 91s 134us/step - loss: 0.1305 - acc: 0.9516\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1287 - acc: 0.9517\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1280 - acc: 0.9521\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1270 - acc: 0.9525\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1262 - acc: 0.9529\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1253 - acc: 0.9531\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1254 - acc: 0.9529\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1245 - acc: 0.9532\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1239 - acc: 0.9532\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 87s 129us/step - loss: 0.1239 - acc: 0.9533\n",
            "75221/75221 [==============================] - 13s 171us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 95.28854974017842\n",
            "Loss: 12.80028699594003\n",
            "------Fold: 9------\n",
            "Epoch 1/10\n",
            "676993/676993 [==============================] - 91s 134us/step - loss: 0.1264 - acc: 0.9527\n",
            "Epoch 2/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1249 - acc: 0.9531\n",
            "Epoch 3/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1242 - acc: 0.9535\n",
            "Epoch 4/10\n",
            "676993/676993 [==============================] - 86s 127us/step - loss: 0.1229 - acc: 0.9539\n",
            "Epoch 5/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1230 - acc: 0.9537\n",
            "Epoch 6/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1232 - acc: 0.9535\n",
            "Epoch 7/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1216 - acc: 0.9542\n",
            "Epoch 8/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1213 - acc: 0.9542\n",
            "Epoch 9/10\n",
            "676993/676993 [==============================] - 86s 128us/step - loss: 0.1211 - acc: 0.9544\n",
            "Epoch 10/10\n",
            "676993/676993 [==============================] - 87s 128us/step - loss: 0.1198 - acc: 0.9548\n",
            "75221/75221 [==============================] - 13s 174us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 95.32444397192168\n",
            "Loss: 12.421753959889562\n",
            "------Fold: 10------\n",
            "Epoch 1/10\n",
            "676989/676989 [==============================] - 91s 135us/step - loss: 0.1230 - acc: 0.9537\n",
            "Epoch 2/10\n",
            "676989/676989 [==============================] - 87s 128us/step - loss: 0.1214 - acc: 0.9544\n",
            "Epoch 3/10\n",
            "676989/676989 [==============================] - 87s 129us/step - loss: 0.1206 - acc: 0.9545\n",
            "Epoch 4/10\n",
            "676989/676989 [==============================] - 89s 131us/step - loss: 0.1198 - acc: 0.9549\n",
            "Epoch 5/10\n",
            "676989/676989 [==============================] - 89s 132us/step - loss: 0.1196 - acc: 0.9548\n",
            "Epoch 6/10\n",
            "676989/676989 [==============================] - 89s 132us/step - loss: 0.1189 - acc: 0.9552\n",
            "Epoch 7/10\n",
            "676989/676989 [==============================] - 89s 132us/step - loss: 0.1187 - acc: 0.9552\n",
            "Epoch 8/10\n",
            "676989/676989 [==============================] - 89s 131us/step - loss: 0.1184 - acc: 0.9555\n",
            "Epoch 9/10\n",
            "676989/676989 [==============================] - 89s 132us/step - loss: 0.1170 - acc: 0.9558\n",
            "Epoch 10/10\n",
            "676989/676989 [==============================] - 89s 132us/step - loss: 0.1167 - acc: 0.9559\n",
            "75221/75221 [==============================] - 14s 186us/step\n",
            "-------------Test Score ----------\n",
            "Accuracy: 95.2672790843775\n",
            "Loss: 12.532806696167665\n",
            "94.48% (+/- 0.76%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z_5oAX1a-Vq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}